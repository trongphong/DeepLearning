{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntphong/miniconda3/envs/mlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_point=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "token_acc='' \n",
    "#'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "#'microsoft/DialoGPT-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(check_point,torch_dtype=torch.bfloat16, token=token_acc).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(check_point,token=token_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_chat(text, sys_prompt):\n",
    "    messages =[ \n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    messages_token=tokenizer.apply_chat_template( \n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "        ).to(device)\n",
    "    messages_gen=model.generate(\n",
    "        messages_token,\n",
    "        max_length=1000,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        temperature=0.0001,\n",
    "    )\n",
    "    ouput=tokenizer.decode(messages_gen[0], skip_special_tokens=True)\n",
    "    return ouput.split('assistant')[-1]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nParis is the capital city of France, a country located in Western Europe. It is situated in the Île-de-France region, which is one of the largest metropolitan areas in Europe.\\n\\nParis is located in the northwestern part of France, approximately 300 kilometers (186 miles) northwest of the English Channel. The city is nestled between the Seine River and the suburbs of Paris, which are home to many of the city's famous landmarks, including the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\\n\\nHere's a rough outline of Paris's location:\\n\\n* Latitude: 48.8567° N\\n* Longitude: 2.3522° E\\n* Region: Île-de-France\\n* Country: France\\n* Country code: +33\\n* Airport: Charles de Gaulle Airport (CDG)\\n\\nParis is easily accessible by air, with two major airports: Charles de Gaulle Airport (CDG) and Orly Airport (ORY). The city is also connected to other major European cities by high-speed trains, such as the TGV (Train à Grande Vitesse) and the Eurostar.\\n\\nI hope that helps you visualize the location of Paris!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_chat('where is Paris', 'chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntphong/miniconda3/envs/mlenv/lib/python3.10/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "demo=gr.ChatInterface(\n",
    "    fn=func_chat,\n",
    "    #type='messages',\n",
    "    textbox=gr.Textbox(placeholder='Enter question here', container=False, scale=7),\n",
    "    chatbot=gr.Chatbot(height=400),    \n",
    ")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
